{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de2ec1ee2fb74c4abd1a6c417e39d5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29318931f0a645c7b0cdef201c241b8f",
              "IPY_MODEL_437af75f85644b3e8503556d10e263e5",
              "IPY_MODEL_9f69b8602403493997fa96cce7d3bd16"
            ],
            "layout": "IPY_MODEL_8aece554078a401292935f0e6e9fe0ad"
          }
        },
        "29318931f0a645c7b0cdef201c241b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9ac795cb1ea415680fc6f2cc2a058c1",
            "placeholder": "​",
            "style": "IPY_MODEL_332c15254ded443799c72104c07cf29e",
            "value": "Batches: 100%"
          }
        },
        "437af75f85644b3e8503556d10e263e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f65baa0afbe04e72a32a5e66188f5a45",
            "max": 14,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e8261ec8e7749f083099688326f02cc",
            "value": 14
          }
        },
        "9f69b8602403493997fa96cce7d3bd16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4a4ab994b1c4e21947b148a087fb4f9",
            "placeholder": "​",
            "style": "IPY_MODEL_a16e437a8d484844b63d1c6728b9aa62",
            "value": " 14/14 [01:03&lt;00:00,  3.19s/it]"
          }
        },
        "8aece554078a401292935f0e6e9fe0ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9ac795cb1ea415680fc6f2cc2a058c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "332c15254ded443799c72104c07cf29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f65baa0afbe04e72a32a5e66188f5a45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e8261ec8e7749f083099688326f02cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4a4ab994b1c4e21947b148a087fb4f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a16e437a8d484844b63d1c6728b9aa62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf sentence-transformers faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "R6yjOJupbDWo",
        "outputId": "085c8ca8-abef-4dbf-83f0-89b9d1f86fc5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import textwrap\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import torch\n",
        "\n",
        "\n",
        "class MultiDocRetriever:\n",
        "    \"\"\"\n",
        "    Minimal but robust implementation of:\n",
        "    - PDF ingestion\n",
        "    - text cleaning + chunking\n",
        "    - embedding computation\n",
        "    - FAISS-based vector search\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"all-MiniLM-L6-v2\",\n",
        "        max_chars: int = 800,\n",
        "        overlap_chars: int = 150,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the retriever.\n",
        "\n",
        "        Args:\n",
        "            model_name: SentenceTransformer model name.\n",
        "            max_chars: Max number of characters per chunk.\n",
        "            overlap_chars: Overlap between consecutive chunks\n",
        "                           (helps preserve context across boundaries).\n",
        "        \"\"\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.max_chars = max_chars\n",
        "        self.overlap_chars = overlap_chars\n",
        "\n",
        "        self.chunks: List[str] = []\n",
        "        self.meta: List[Dict] = []   # per-chunk metadata\n",
        "        self.embeddings: Optional[np.ndarray] = None\n",
        "        self.index: Optional[faiss.Index] = None\n",
        "\n",
        "\n",
        "    # 1. PDF loading w. the text prep\n",
        "    @staticmethod\n",
        "    def _load_pdf_text(pdf_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Read all text from a PDF file.\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Path to the PDF file.\n",
        "\n",
        "        Returns:\n",
        "            Raw extracted text as a single string.\n",
        "        \"\"\"\n",
        "        reader = PdfReader(pdf_path)\n",
        "        texts = []\n",
        "        for page in reader.pages:\n",
        "            try:\n",
        "                page_text = page.extract_text() or \"\"\n",
        "            except Exception:\n",
        "                page_text = \"\"\n",
        "            texts.append(page_text)\n",
        "        return \"\\n\".join(texts)\n",
        "\n",
        "    @staticmethod\n",
        "    def _clean_text(text: str) -> str:\n",
        "        \"\"\"\n",
        "        Basic text cleaning.\n",
        "\n",
        "        - Replace newlines with spaces.\n",
        "        - Collapse multiple whitespace into a single space.\n",
        "        - Strip leading/trailing whitespace.\n",
        "        \"\"\"\n",
        "        cleaned = \" \".join(text.replace(\"\\r\", \" \").replace(\"\\n\", \" \").split())\n",
        "        return cleaned.strip()\n",
        "\n",
        "    def _chunk_text(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into overlapping chunks by character length.\n",
        "\n",
        "        Args:\n",
        "            text: Cleaned text.\n",
        "\n",
        "        Returns:\n",
        "            List of text chunks.\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        n = len(text)\n",
        "\n",
        "        # Sliding window with overlap\n",
        "        while start < n:\n",
        "            end = min(start + self.max_chars, n)\n",
        "            chunk = text[start:end].strip()\n",
        "            if chunk:\n",
        "                chunks.append(chunk)\n",
        "            if end == n:\n",
        "                break\n",
        "            # Move back by overlap_chars to keep some context\n",
        "            start = end - self.overlap_chars\n",
        "\n",
        "        return chunks\n",
        "\n",
        "\n",
        "    # 2. Ingestion\n",
        "    def add_pdf(self, pdf_path: str, doc_id: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Ingest a single PDF: load, clean, chunk, and store metadata.\n",
        "        Embeddings/index are NOT built here (call build_index separately).\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Path to PDF.\n",
        "            doc_id: Optional explicit document ID (e.g., \"paper1\").\n",
        "                    If None, use the filename.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(pdf_path):\n",
        "            raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
        "\n",
        "        if doc_id is None:\n",
        "            doc_id = os.path.basename(pdf_path)\n",
        "\n",
        "        raw_text = self._load_pdf_text(pdf_path)\n",
        "        cleaned = self._clean_text(raw_text)\n",
        "        chunks = self._chunk_text(cleaned)\n",
        "\n",
        "        start_idx = len(self.chunks)\n",
        "        for local_idx, chunk in enumerate(chunks):\n",
        "            self.chunks.append(chunk)\n",
        "            self.meta.append(\n",
        "                {\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"chunk_id\": local_idx,\n",
        "                    \"global_id\": start_idx + local_idx,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(f\"[INFO] Ingested {doc_id}: {len(chunks)} chunks.\")\n",
        "\n",
        "    def add_pdfs_from_dir(self, pdf_dir: str, recursive: bool = False):\n",
        "        \"\"\"\n",
        "        Ingest all PDFs from a directory.\n",
        "\n",
        "        Args:\n",
        "            pdf_dir: Directory containing PDF files.\n",
        "            recursive: If True, walk subdirectories as well.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(pdf_dir):\n",
        "            raise NotADirectoryError(f\"Directory not found: {pdf_dir}\")\n",
        "\n",
        "        count_files = 0\n",
        "        for root, dirs, files in os.walk(pdf_dir):\n",
        "            for fname in files:\n",
        "                if fname.lower().endswith(\".pdf\"):\n",
        "                    fpath = os.path.join(root, fname)\n",
        "                    self.add_pdf(fpath)\n",
        "                    count_files += 1\n",
        "            if not recursive:\n",
        "                break\n",
        "\n",
        "        print(f\"[INFO] Finished ingesting PDFs from {pdf_dir}. Total files: {count_files}\")\n",
        "\n",
        "    # 3. Embeddings + FAISS index\n",
        "    def build_index(self, show_progress: bool = True):\n",
        "        \"\"\"\n",
        "        Compute embeddings for all chunks and build a FAISS index.\n",
        "\n",
        "        Args:\n",
        "            show_progress: Whether to show a progress bar during encoding.\n",
        "        \"\"\"\n",
        "        if not self.chunks:\n",
        "            raise ValueError(\"No chunks available. Ingest PDFs before building index.\")\n",
        "\n",
        "        print(f\"[INFO] Computing embeddings for {len(self.chunks)} chunks...\")\n",
        "        self.embeddings = self.model.encode(\n",
        "            self.chunks,\n",
        "            batch_size=32,\n",
        "            convert_to_numpy=True,\n",
        "            show_progress_bar=show_progress,\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        d = self.embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(d)\n",
        "        self.index.add(self.embeddings)\n",
        "        print(f\"[INFO] FAISS index built. Vector dimension = {d}, size = {self.index.ntotal}\")\n",
        "\n",
        "    # 4. Retrieval\n",
        "    def retrieve(self, query: str, k: int = 5) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Retrieve top-k most relevant chunks for a query.\n",
        "\n",
        "        Args:\n",
        "            query: Natural language question / query string.\n",
        "            k: Number of chunks to return.\n",
        "\n",
        "        Returns:\n",
        "            List of dicts with: rank, score (distance), doc_id, chunk_id, text\n",
        "        \"\"\"\n",
        "        if self.index is None or self.embeddings is None:\n",
        "            raise ValueError(\"Index not built. Call build_index() first.\")\n",
        "\n",
        "        if not query.strip():\n",
        "            raise ValueError(\"Query is empty.\")\n",
        "\n",
        "        q_emb = self.model.encode(\n",
        "            [query],\n",
        "            convert_to_numpy=True,\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        distances, indices = self.index.search(q_emb, k)\n",
        "        indices = indices[0]\n",
        "        distances = distances[0]\n",
        "\n",
        "        results = []\n",
        "        for rank, (idx, dist) in enumerate(zip(indices, distances), start=1):\n",
        "            meta = self.meta[idx]\n",
        "            results.append(\n",
        "                {\n",
        "                    \"rank\": rank,\n",
        "                    \"score\": float(dist),  # smaller = more similar for L2\n",
        "                    \"doc_id\": meta[\"doc_id\"],\n",
        "                    \"chunk_id\": int(meta[\"chunk_id\"]),\n",
        "                    \"global_id\": int(meta[\"global_id\"]),\n",
        "                    \"text\": self.chunks[idx],\n",
        "                }\n",
        "            )\n",
        "        return results\n",
        "\n",
        "    # 5. Save and load index and metadata\n",
        "    def save(self, out_dir: str):\n",
        "        \"\"\"\n",
        "        Save embeddings, metadata, and FAISS index to disk.\n",
        "\n",
        "        Args:\n",
        "            out_dir: Directory to save files into.\n",
        "        \"\"\"\n",
        "        if self.embeddings is None or self.index is None:\n",
        "            raise ValueError(\"Nothing to save. Build the index first.\")\n",
        "\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "        # Save embeddings\n",
        "        np.save(os.path.join(out_dir, \"embeddings.npy\"), self.embeddings)\n",
        "\n",
        "        # Save chunks and metadata\n",
        "        with open(os.path.join(out_dir, \"chunks.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.chunks, f, ensure_ascii=False)\n",
        "\n",
        "        with open(os.path.join(out_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.meta, f, ensure_ascii=False)\n",
        "\n",
        "        # Save FAISS index\n",
        "        faiss.write_index(self.index, os.path.join(out_dir, \"faiss.index\"))\n",
        "\n",
        "        print(f\"[INFO] Saved index and metadata to {out_dir}\")\n",
        "\n",
        "    def load(self, in_dir: str):\n",
        "        \"\"\"\n",
        "        Load embeddings, metadata, and FAISS index from disk.\n",
        "\n",
        "        Args:\n",
        "            in_dir: Directory from which to load files.\n",
        "        \"\"\"\n",
        "        emb_path = os.path.join(in_dir, \"embeddings.npy\")\n",
        "        chunks_path = os.path.join(in_dir, \"chunks.json\")\n",
        "        meta_path = os.path.join(in_dir, \"meta.json\")\n",
        "        index_path = os.path.join(in_dir, \"faiss.index\")\n",
        "\n",
        "        if not all(os.path.exists(p) for p in [emb_path, chunks_path, meta_path, index_path]):\n",
        "            raise FileNotFoundError(\"Missing one or more saved files in the directory.\")\n",
        "\n",
        "        self.embeddings = np.load(emb_path)\n",
        "\n",
        "        with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.chunks = json.load(f)\n",
        "\n",
        "        with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.meta = json.load(f)\n",
        "\n",
        "        self.index = faiss.read_index(index_path)\n",
        "        print(f\"[INFO] Loaded index and metadata from {in_dir}\")\n",
        "\n",
        "\n",
        "# Helper functions built on top of MultiDocRetriever\n",
        "\n",
        "def summarize_corpus(retriever: MultiDocRetriever) -> None:\n",
        "    \"\"\"\n",
        "    Print a short summary of the corpus:\n",
        "    total number of chunks and number of chunks per document.\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    counts = Counter(m[\"doc_id\"] for m in retriever.meta)\n",
        "    print(f\"Total chunks: {len(retriever.chunks)}\")\n",
        "    for doc_id, c in counts.items():\n",
        "        print(f\"- {doc_id}: {c} chunks\")\n",
        "\n",
        "\n",
        "def get_context_for_query(\n",
        "    retriever: MultiDocRetriever,\n",
        "    query: str,\n",
        "    k: int = 6,\n",
        "    max_total_chars: int = 4000,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Retrieve top-k chunks and format them as a single context string\n",
        "    suitable for feeding into an LLM prompt.\n",
        "\n",
        "    Args:\n",
        "        retriever: A fitted MultiDocRetriever with an index built.\n",
        "        query: Natural language question / query string.\n",
        "        k: Number of chunks to retrieve.\n",
        "        max_total_chars: Soft limit on total context length (in characters).\n",
        "\n",
        "    Returns:\n",
        "        A concatenated string of chunks with simple headers.\n",
        "    \"\"\"\n",
        "    chunks = retriever.retrieve(query, k=k)\n",
        "    parts = []\n",
        "    total = 0\n",
        "\n",
        "    for c in chunks:\n",
        "        header = f\"[{c['doc_id']} — chunk {c['chunk_id']}]\"\n",
        "        block = header + \"\\n\" + c[\"text\"]\n",
        "        if total + len(block) > max_total_chars:\n",
        "            break\n",
        "        parts.append(block)\n",
        "        total += len(block)\n",
        "\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "\n",
        "def demo_retrieval_example(retriever: MultiDocRetriever) -> None:\n",
        "    \"\"\"\n",
        "    Simple demo: run a fixed query and print the top-k chunks.\n",
        "    \"\"\"\n",
        "    user_question = \"What are the main risks discussed in this paper?\"\n",
        "    results = retriever.retrieve(user_question, k=6)\n",
        "    for r in results:\n",
        "        print(f\"[{r['rank']}] {r['doc_id']} (chunk {r['chunk_id']}), score={r['score']:.4f}\")\n",
        "        print(r[\"text\"][:400], \"\\n\", \"-\" * 80)\n",
        "\n"
      ],
      "metadata": {
        "id": "aSjKDy7QbEQE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "os.makedirs(\"papers\", exist_ok=True)\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fname in uploaded.keys():\n",
        "    os.rename(fname, f\"papers/{fname}\")\n",
        "\n",
        "os.listdir(\"papers\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "usEGtKH1bFqQ",
        "outputId": "aa4457d1-efc0-4a34-d60a-73b08f5dbe78"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a0818b73-bea5-49f5-99ec-d8904ded07c6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a0818b73-bea5-49f5-99ec-d8904ded07c6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 135 Term Structure Review ARFE 2009.pdf to 135 Term Structure Review ARFE 2009 (1).pdf\n",
            "Saving 248 Inflation ARFE 2023.pdf to 248 Inflation ARFE 2023 (1).pdf\n",
            "Saving 259 Coupon Bond JFQA 2024 wp.pdf to 259 Coupon Bond JFQA 2024 wp (1).pdf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['248 Inflation ARFE 2023 (1).pdf',\n",
              " '135 Term Structure Review ARFE 2009 (1).pdf',\n",
              " '259 Coupon Bond JFQA 2024 wp (1).pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.retriever\n",
        "retriever = MultiDocRetriever(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    max_chars=800,\n",
        "    overlap_chars=150,\n",
        ")\n",
        "\n",
        "# 2. Ingest all PDFs in the \"papers\" folder\n",
        "retriever.add_pdfs_from_dir(\"papers\", recursive=False)\n",
        "\n",
        "# 3. Build vector index\n",
        "retriever.build_index(show_progress=True)\n",
        "\n",
        "# 4. Summarize\n",
        "summarize_corpus(retriever)\n",
        "\n",
        "# 5. Run a structured demo\n",
        "demo_retrieval_example(retriever)\n",
        "\n",
        "# 6. Build context for LLM (Module #2 will use this)\n",
        "ctx = get_context_for_query(\n",
        "    retriever,\n",
        "    \"What are the main risks discussed in this paper?\",\n",
        "    k=6\n",
        ")\n",
        "\n",
        "print(ctx[:800])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605,
          "referenced_widgets": [
            "de2ec1ee2fb74c4abd1a6c417e39d5eb",
            "29318931f0a645c7b0cdef201c241b8f",
            "437af75f85644b3e8503556d10e263e5",
            "9f69b8602403493997fa96cce7d3bd16",
            "8aece554078a401292935f0e6e9fe0ad",
            "b9ac795cb1ea415680fc6f2cc2a058c1",
            "332c15254ded443799c72104c07cf29e",
            "f65baa0afbe04e72a32a5e66188f5a45",
            "3e8261ec8e7749f083099688326f02cc",
            "c4a4ab994b1c4e21947b148a087fb4f9",
            "a16e437a8d484844b63d1c6728b9aa62"
          ]
        },
        "id": "kjp9jjP9bJhX",
        "outputId": "accb9ce6-08f5-4a34-fc29-7c748c5f9a26"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Ingested 248 Inflation ARFE 2023 (1).pdf: 116 chunks.\n",
            "[INFO] Ingested 135 Term Structure Review ARFE 2009 (1).pdf: 125 chunks.\n",
            "[INFO] Ingested 259 Coupon Bond JFQA 2024 wp (1).pdf: 179 chunks.\n",
            "[INFO] Finished ingesting PDFs from papers. Total files: 3\n",
            "[INFO] Computing embeddings for 420 chunks...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de2ec1ee2fb74c4abd1a6c417e39d5eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] FAISS index built. Vector dimension = 384, size = 420\n",
            "Total chunks: 420\n",
            "- 248 Inflation ARFE 2023 (1).pdf: 116 chunks\n",
            "- 135 Term Structure Review ARFE 2009 (1).pdf: 125 chunks\n",
            "- 259 Coupon Bond JFQA 2024 wp (1).pdf: 179 chunks\n",
            "[1] 259 Coupon Bond JFQA 2024 wp (1).pdf (chunk 65), score=1.2674\n",
            "a vector of constants. For an application of such a hazard rate model applied to corporate default probabilities see Chava and Jarrow (2004). As discussed in Jarrow, Lando, and Yu (2005), this assumption does not imply that risky coupon bonds earn no risk premium. Quite the contrary. If the state variables Γt driving the default process represent systematic risk, which is the most likely case, the \n",
            " --------------------------------------------------------------------------------\n",
            "[2] 259 Coupon Bond JFQA 2024 wp (1).pdf (chunk 67), score=1.2705\n",
            "robabili- ties from the 10-year term structure of monthly marginal default probabilities (the monthly probability of default conditional on no prior default). The state variables used in KRIS’s hazard rate estimation include both firm specific and macroeconomic variables. Importantly, 23See www.kamakuraco.com. 24The model underlying the default probability calculations is similar to the one used i \n",
            " --------------------------------------------------------------------------------\n",
            "[3] 135 Term Structure Review ARFE 2009 (1).pdf (chunk 124), score=1.2894\n",
            "ition Densities Yacine Aı¨t-Sahalia ..........................................3 4 1 Learning in Financial Markets Lubos Pastor and Pietro Veronesi ...............................3 6 1 What Decision Neuroscience Teaches Us About Financial Decision Making Peter Bossaerts .............................................3 8 3 Errata An online log of corrections toAnnual Review of Financial Economics arti \n",
            " --------------------------------------------------------------------------------\n",
            "[4] 135 Term Structure Review ARFE 2009 (1).pdf (chunk 123), score=1.2914\n",
            ".......2 0 7 Consumer Finance Peter Tufano ..............................................2 2 7 Life-Cycle Finance and the Design of Pension Plans Zvi Bodie, Je´roˆme Detemple, and Marcel Rindisbacher ..............2 4 9 Finance and Inequality: Theory and Evidence Asli Demirgu¨ c¸-Kunt and Ross Levine ............................2 8 7 Volume 1, 2009 v Annu. Rev. Fin. Econ. 2009.1:69-96. Downloaded  \n",
            " --------------------------------------------------------------------------------\n",
            "[5] 135 Term Structure Review ARFE 2009 (1).pdf (chunk 88), score=1.3005\n",
            "hao (2006), Long- staff et al. (2001), Jarrow et al. (2007), and Gupta & Subrahmanyam (2005). A summary of the empirical evidence is contained in Rebonato (2002). 9. APPLICATIONS TO OTHER MARKETS The HJM model is the building block for pricing and hedging all fixed-income securities, including those with credit risk. One important application has been to the risk manage- ment of mortgages, both re \n",
            " --------------------------------------------------------------------------------\n",
            "[6] 135 Term Structure Review ARFE 2009 (1).pdf (chunk 93), score=1.3040\n",
            "ues. Settling these issues is essential for understanding risk management for both corporate and financial institutions. The capital structure decision for a corporation, that is, the determination of the debt equity ratio, or the determination of economic capital for a financial institution (related to the Basel II Accord), depends crucially on the evolution of the term structure of interest rate \n",
            " --------------------------------------------------------------------------------\n",
            "[259 Coupon Bond JFQA 2024 wp (1).pdf — chunk 65]\n",
            "a vector of constants. For an application of such a hazard rate model applied to corporate default probabilities see Chava and Jarrow (2004). As discussed in Jarrow, Lando, and Yu (2005), this assumption does not imply that risky coupon bonds earn no risk premium. Quite the contrary. If the state variables Γt driving the default process represent systematic risk, which is the most likely case, then risky coupon bond prices necessarily earn a risk premium due to the bond price’s correlation to Γt. The diversifiable risk assumption just states that the timing of the default event itself, after conditioning onΓt, is diversifiable in a large portfolio. Alternatively stated, in a poor economy all firms are more likely to default. But, the timing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"What are the main risks discussed in this paper?\"\n",
        "context = get_context_for_query(retriever, user_question, k=6)\n",
        "\n",
        "print(context[:800])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hPUWJ-AObLSB",
        "outputId": "e2b11485-9358-40a2-ff44-4d1a58398291"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[259 Coupon Bond JFQA 2024 wp (1).pdf — chunk 65]\n",
            "a vector of constants. For an application of such a hazard rate model applied to corporate default probabilities see Chava and Jarrow (2004). As discussed in Jarrow, Lando, and Yu (2005), this assumption does not imply that risky coupon bonds earn no risk premium. Quite the contrary. If the state variables Γt driving the default process represent systematic risk, which is the most likely case, then risky coupon bond prices necessarily earn a risk premium due to the bond price’s correlation to Γt. The diversifiable risk assumption just states that the timing of the default event itself, after conditioning onΓt, is diversifiable in a large portfolio. Alternatively stated, in a poor economy all firms are more likely to default. But, the timing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context_for_query(retriever, query: str, k: int = 6) -> str:\n",
        "    \"\"\"\n",
        "    Retrieve top-k chunks and format them as a single context string\n",
        "    suitable for feeding into an LLM prompt.\n",
        "    \"\"\"\n",
        "    chunks = retriever.retrieve(query, k=k)\n",
        "    parts = []\n",
        "    for c in chunks:\n",
        "        header = f\"[{c['doc_id']} — chunk {c['chunk_id']}]\"\n",
        "        parts.append(header + \"\\n\" + c[\"text\"])\n",
        "    return \"\\n\\n\".join(parts)\n"
      ],
      "metadata": {
        "id": "Qp1V3UPedUzG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.save(\"multi_doc_index\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6vo-rXwbdXYV",
        "outputId": "a03a0d34-75ee-4dd7-89ac-e1980ddbf2cb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saved index and metadata to multi_doc_index\n"
          ]
        }
      ]
    }
  ]
}