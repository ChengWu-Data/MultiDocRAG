{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "FPWOUekMx0WA"
      },
      "outputs": [],
      "source": [
        "# If you already installed these in 01_ingestion_retrieval, you can skip.\n",
        "!pip install -q transformers sentence-transformers faiss-cpu pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up project path (Colab + local)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def in_colab():\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if in_colab():\n",
        "    # Clone the repo once per runtime\n",
        "    if not os.path.exists(\"/content/MultiDocRAG\"):\n",
        "        !git clone https://github.com/ChengWu-Data/MultiDocRAG.git\n",
        "    %cd /content/MultiDocRAG\n",
        "\n",
        "PROJECT_ROOT = os.getcwd()\n",
        "sys.path.append(PROJECT_ROOT)\n",
        "\n",
        "print(\"Project root:\", PROJECT_ROOT)\n",
        "print(\"Dir listing:\", os.listdir(PROJECT_ROOT))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "R7zo0-KjqR9r",
        "outputId": "3382f8ca-b881-49b1-a92c-57aec541ae82"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MultiDocRAG\n",
            "Project root: /content/MultiDocRAG\n",
            "Dir listing: ['notebooks', 'index_store', 'src', '.gitignore', 'LICENSE', '.git', 'README.md']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load retriever + index from module #1\n",
        "\n",
        "from src.retriever import MultiDocRetriever\n",
        "\n",
        "INDEX_DIR = os.path.join(PROJECT_ROOT, \"index_store\")\n",
        "\n",
        "retriever = MultiDocRetriever(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    max_chars=800,\n",
        "    overlap_chars=150,\n",
        ")\n",
        "retriever.load(INDEX_DIR)\n",
        "\n",
        "print(\"Index loaded from:\", INDEX_DIR)\n",
        "print(\"Total chunks:\", len(retriever.chunks))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "N_lEMigAqS3F",
        "outputId": "fa176637-d8dc-4a8e-8eb7-0b678d99c33e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loaded index from /content/MultiDocRAG/index_store\n",
            "Index loaded from: /content/MultiDocRAG/index_store\n",
            "Total chunks: 420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions for RAG (context + prompt)\n",
        "\n",
        "def get_context_for_query(\n",
        "    retriever: MultiDocRetriever,\n",
        "    question: str,\n",
        "    k: int = 6,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Retrieve top-k chunks and format them into a single context string.\n",
        "    \"\"\"\n",
        "    chunks = retriever.retrieve(question, k=k)\n",
        "    blocks = []\n",
        "    for c in chunks:\n",
        "        header = f\"[{c['doc_id']} — chunk {c['chunk_id']}]\"\n",
        "        blocks.append(header + \"\\n\" + c[\"text\"])\n",
        "    return \"\\n\\n\".join(blocks)\n",
        "\n",
        "\n",
        "def build_rag_prompt(question: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Build the prompt given retrieved context + user question.\n",
        "    The goal is to keep the model grounded in the provided text.\n",
        "    \"\"\"\n",
        "    return f\"\"\"You are a teaching assistant for a graduate-level financial economics course.\n",
        "\n",
        "You are given several excerpts from academic finance papers.\n",
        "Use ONLY this context to answer the question.\n",
        "If the context does not contain enough information, say:\n",
        "\"The context does not provide enough information to answer this fully.\"\n",
        "Do not introduce facts that are not supported by the context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Provide a concise answer in 1–2 short paragraphs.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "fzE9IUIuqVBX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load open-source language model (local inference, no API)\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# u can switch this to another instruction model if needed!\n",
        "# Some common open-source options (not all will fit on free Colab GPU):\n",
        "#   - mistralai/Mistral-7B-Instruct-v0.2\n",
        "#   - meta-llama/Meta-Llama-3-8B-Instruct\n",
        "#   - google/gemma-2-7b-it\n",
        "#   - microsoft/Phi-3-mini-4k-instruct\n",
        "#   - TinyLlama/TinyLlama-1.1B-Chat-v1.0   (usually OK on Colab Pro)\n",
        "#\n",
        "# For now I m using a very small model so the notebook actually runs on\n",
        "# the default Colab environment. Performance is not the focus here — the\n",
        "# goal is just to demonstrate the RAG pipeline end-to-end.\n",
        "\n",
        "MODEL_NAME = \"sshleifer/tiny-gpt2\"\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def get_max_context_len(model) -> int:\n",
        "    \"\"\"\n",
        "    Infer the maximum context length (in tokens) of the model from its config.\n",
        "    This works for GPT-2 style models and most other Hugging Face causal LMs.\n",
        "    \"\"\"\n",
        "    cfg = model.config\n",
        "    for name in [\n",
        "        \"max_position_embeddings\",  # most Transformer models\n",
        "        \"n_positions\",              # GPT-2 family\n",
        "        \"max_seq_len\",\n",
        "        \"max_sequence_length\",\n",
        "        \"seq_length\",\n",
        "    ]:\n",
        "        if hasattr(cfg, name) and getattr(cfg, name) is not None:\n",
        "            try:\n",
        "                return int(getattr(cfg, name))\n",
        "            except (TypeError, ValueError):\n",
        "                pass\n",
        "\n",
        "    # Conservative fallback if nothing is found\n",
        "    return 1024\n",
        "\n",
        "\n",
        "# Make sure the tokenizer has a pad token (GPT-2 does not by default).\n",
        "if tokenizer.pad_token_id is None:\n",
        "    if tokenizer.eos_token_id is not None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    else:\n",
        "        # Fallback: reuse unk as pad; rarely matters for small demos.\n",
        "        tokenizer.add_special_tokens({\"pad_token\": tokenizer.unk_token})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "max_ctx = get_max_context_len(model)\n",
        "print(\"Model loaded.\")\n",
        "print(\"Max context length:\", max_ctx)\n",
        "print(\"Pad token id:\", tokenizer.pad_token_id)\n",
        "print(\"EOS token id:\", tokenizer.eos_token_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Y8RQ5LxnvcmT",
        "outputId": "21e1bbe1-b322-4a70-ed27-af2bccba491e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: sshleifer/tiny-gpt2\n",
            "Model loaded.\n",
            "Max context length: 1024\n",
            "Pad token id: 50256\n",
            "EOS token id: 50256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_context_len(model) -> int:\n",
        "    \"\"\"\n",
        "    Infer the maximum context length of the model from its config.\n",
        "    This works for GPT-2 style models and most other HF causal LMs.\n",
        "    \"\"\"\n",
        "    cfg = model.config\n",
        "    for name in [\n",
        "        \"max_position_embeddings\",  # most Transformer models\n",
        "        \"n_positions\",              # GPT-2 family\n",
        "        \"max_seq_len\",\n",
        "        \"max_sequence_length\",\n",
        "        \"seq_length\",\n",
        "    ]:\n",
        "        if hasattr(cfg, name) and getattr(cfg, name) is not None:\n",
        "            try:\n",
        "                return int(getattr(cfg, name))\n",
        "            except (TypeError, ValueError):\n",
        "                pass\n",
        "\n",
        "    # Conservative fallback\n",
        "    return 1024"
      ],
      "metadata": {
        "id": "Sw3Fk7cttd-r"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "def generate_from_model(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 256,\n",
        "    temperature: float = 0.7,\n",
        "    top_p: float = 0.95,\n",
        "    top_k: Optional[int] = None,\n",
        "    return_only_new: bool = False,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Run a single forward pass of the causal LM.\n",
        "\n",
        "    Features:\n",
        "    - Respects the model's max context length.\n",
        "    - Ensures input length + generated tokens never exceed that limit.\n",
        "    - Supports temperature / top-p (nucleus) / top-k sampling.\n",
        "    - Falls back to greedy decoding when no randomness is requested.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get max context length\n",
        "    max_ctx = get_max_context_len(model)\n",
        "\n",
        "    # Tokenize *without* truncation first\n",
        "    enc = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=False,\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"]          # shape: [1, L]\n",
        "    attn_mask = enc.get(\"attention_mask\", None)\n",
        "    input_len = input_ids.shape[1]\n",
        "\n",
        "    # If the prompt alone is too long, keep only the last (max_ctx - 1) tokens\n",
        "    # so that we still have room to generate at least 1 new token.\n",
        "    if input_len >= max_ctx:\n",
        "        keep_len = max_ctx - 1\n",
        "        if keep_len <= 0:\n",
        "            keep_len = 1  # extreme edge case\n",
        "        input_ids = input_ids[:, -keep_len:]\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask[:, -keep_len:]\n",
        "        input_len = keep_len\n",
        "\n",
        "    # Enforce: input_len + max_new_tokens <= max_ctx.\n",
        "    available_for_gen = max_ctx - input_len\n",
        "    if available_for_gen <= 0:\n",
        "        # No room left, force at least 1 token generation.\n",
        "        max_new_tokens = 1\n",
        "    else:\n",
        "        max_new_tokens = min(max_new_tokens, available_for_gen)\n",
        "\n",
        "    device = model.device\n",
        "    input_ids = input_ids.to(device)\n",
        "    if attn_mask is not None:\n",
        "        attn_mask = attn_mask.to(device)\n",
        "\n",
        "    inputs = {\"input_ids\": input_ids}\n",
        "    if attn_mask is not None:\n",
        "        inputs[\"attention_mask\"] = attn_mask\n",
        "\n",
        "    # Decide whether to sample or use greedy decoding.\n",
        "    use_sampling = False\n",
        "    if temperature is not None and temperature > 0.0:\n",
        "        use_sampling = True\n",
        "    if top_p is not None and top_p < 1.0:\n",
        "        use_sampling = True\n",
        "    if top_k is not None and top_k > 0:\n",
        "        use_sampling = True\n",
        "\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    eos_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else pad_token_id\n",
        "\n",
        "    gen_kwargs = dict(\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=pad_token_id,\n",
        "        eos_token_id=eos_token_id,\n",
        "    )\n",
        "\n",
        "    if use_sampling:\n",
        "        # Nucleus / top-k sampling (this is your \"tup\" = top-p).\n",
        "        gen_kwargs.update(\n",
        "            dict(\n",
        "                do_sample=True,\n",
        "                temperature=max(1e-5, float(temperature)),\n",
        "                top_p=float(top_p) if top_p is not None else 1.0,\n",
        "                top_k=int(top_k) if top_k is not None else 0,\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        # Pure greedy decoding (no sampling).\n",
        "        gen_kwargs.update(\n",
        "            dict(\n",
        "                do_sample=False,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            **gen_kwargs,\n",
        "        )\n",
        "\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if return_only_new:\n",
        "        original_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "        if full_text.startswith(original_text):\n",
        "            return full_text[len(original_text):].lstrip()\n",
        "\n",
        "    return full_text\n"
      ],
      "metadata": {
        "id": "1ilnjze8qbpI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unified QA interface (baseline vs RAG)\n",
        "\n",
        "def answer_question(\n",
        "    retriever: MultiDocRetriever,\n",
        "    question: str,\n",
        "    mode: str = \"rag\",\n",
        "    k: int = 6,\n",
        "    temperature: float = 0.2,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Unified entry point for QA.\n",
        "\n",
        "    mode = \"rag\": retrieval-augmented generation with multi-document context.\n",
        "    mode = \"baseline\": same model but no retrieved context.\n",
        "    \"\"\"\n",
        "    if mode == \"rag\":\n",
        "        context = get_context_for_query(retriever, question, k=k)\n",
        "        prompt = build_rag_prompt(question, context)\n",
        "    elif mode == \"baseline\":\n",
        "        context = None\n",
        "        prompt = f\"\"\"You are a general-purpose assistant.\n",
        "Answer the question below as best as you can.\n",
        "Do not assume you have access to any specific research papers.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mode: {mode}\")\n",
        "\n",
        "\n",
        "    answer = generate_from_model(\n",
        "        prompt=prompt,\n",
        "        max_new_tokens=256,\n",
        "        temperature=temperature,\n",
        "        top_p=0.95,\n",
        "        top_k=None,\n",
        "        )\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"mode\": mode,\n",
        "        \"question\": question,\n",
        "        \"context\": context,\n",
        "        \"prompt\": prompt,\n",
        "        \"answer\": answer,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "dBjuOXPiqfRq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick demo/compare baseline vs RAG on one question\n",
        "\n",
        "test_question = \"What are the main sources of interest rate risk discussed in these papers?\"\n",
        "\n",
        "rag_result = answer_question(retriever, test_question, mode=\"rag\", k=6, temperature=0.1)\n",
        "baseline_result = answer_question(retriever, test_question, mode=\"baseline\", temperature=0.1)\n",
        "\n",
        "print(\"=== RAG ANSWER ===\")\n",
        "print(rag_result[\"answer\"])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(\"=== BASELINE ANSWER ===\")\n",
        "print(baseline_result[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rUK7VQnlqjHF",
        "outputId": "71bcb7e6-c1db-419c-8e5e-050339063df9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1232 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RAG ANSWER ===\n",
            " process is highly correlated with interest rate movements. This, of course, is the case for interest rate derivatives, where the underlying assets are the interest rates themselves. In response, a class of equilibrium-ba\n",
            "\n",
            "[135 Term Structure Review ARFE 2009 (1).pdf — chunk 93]\n",
            "ues. Settling these issues is essential for understanding risk management for both corporate and financial institutions. The capital structure decision for a corporation, that is, the determination of the debt equity ratio, or the determination of economic capital for a financial institution (related to the Basel II Accord), depends crucially on the evolution of the term structure of interest rates. In this determination there are four risks to be consid- ered: market, credit, liquidity, and operational risk. Interest rate risk is the major compo- nent of market risk. However, these term structure models still need to be extended to include credit, liquidity, and operational risk. Although the abstract model formulation for the credit risk extension is also well understood, its empirical i\n",
            "\n",
            "[135 Term Structure Review ARFE 2009 (1).pdf — chunk 121]\n",
            "anc. Econ.5:177–88 Zagst R. 2002.Interest Rate Management. Berlin: Springer 96 Jarrow Annu. Rev. Fin. Econ. 2009.1:69-96. Downloaded from www.annualreviews.org by Cornell University on 10/06/11. For personal use only. Annual Review of Financial Economics Contents Preface to theAnnual Review of Financial Economics Andrew W. Lo and Robert C. Merton .............................1 An Enjoyable Life Puzzling Over Modern Finance Theory Paul A. Samuelson ...........................................1 9 Credit Risk Models Robert A. Jarrow ............................................3 7 The Term Structure of Interest Rates Robert A. Jarrow ............................................6 9 Financial Crises: Theory and Evidence Franklin Allen, Ana Babus, and Elena Carletti ......................9 7 Model\n",
            "\n",
            "[135 Term Structure Review ARFE 2009 (1).pdf — chunk 66]\n",
            "of forward and futures prices is that interest rates are deterministic (the classical result). 6. THE EXPECTATIONS HYPOTHESIS The classical (macro) economics literature of the term structure of interest rates charac- terizes the possible interest rate risk premia (equilibrium term structures) according to three hypotheses about investor behavior: the market segmentation hypothesis, the liquid- ity preference hypothesis, and the expectations hypothesis. The market segmentation hypothesis, normally associated with Culbertson (1957), is that the equilibrium zero-coupon bond prices of different maturities are determined in isolation of the other maturity bonds by distinct market clienteles demanding payoffs at different horizons. Hence, the relevant risk premia for each maturity bond are dete\n",
            "\n",
            "[135 Term Structure Review ARFE 2009 (1).pdf — chunk 11]\n",
            "urse, is the case for interest rate derivatives, where the underlying assets are the interest rates themselves. In response, a class of equilibrium-based interest rate pricing models was developed by Merton (1970), Vasicek (1977), Brennan & Schwartz (1979), and Cox et al. (1985); the latter is known as the Cox-Ingersoll-Ross (CIR) model. This class of models, known as spot rate models, had two limitations. First, they depended on the interest rate risk premia or, equivalently, the expected return on default-free bonds. This dependence, just as with the option pricing models pre-BSM, made their implementation problematic. Indeed, the estimation of risk premia is very difficult. The reason for this difficulty is that the empiri- cal finance literature has documented that risk premia are nons\n",
            "\n",
            "[135 Term Structure Review ARFE 2009 (1).pdf — chunk 0]\n",
            "The Term Structure of Interest Rates Robert A. Jarrow Johnson Graduate School of Management, Cornell University, Ithaca, New York 14853; email: raj15@cornell.edu Annu. Rev. Financ. Econ. 2009. 1:69–96 First published online as a Review in Advance on October 22, 2009 The Annual Review of Financial Economicsis online at financial.annualreviews.org This article’s doi: 10.1146/annurev.financial.050808.114513 Copyright © 2009 by Annual Reviews. All rights reserved 1941-1367/09/1205-0069$20.00 Key Words arbitrage-free term structures, HJM model, expectations hypothesis, LIBOR model, futures and forward contracts Abstract This paper reviews the term structure of interest rates literature relating to the arbitrage-free pricing and hedging of interest rate derivatives. Term structure theory is emph\n",
            "\n",
            "Question:\n",
            "What are the main sources of interest rate risk discussed in these papers?\n",
            "\n",
            "Provide a concise answer in 1–2 short paragraphs.\n",
            "803\n",
            "\n",
            "================================================================================\n",
            "\n",
            "=== BASELINE ANSWER ===\n",
            "You are a general-purpose assistant.\n",
            "Answer the question below as best as you can.\n",
            "Do not assume you have access to any specific research papers.\n",
            "\n",
            "Question:\n",
            "What are the main sources of interest rate risk discussed in these papers?\n",
            " Encounter Transformation drainageDOSheimendrarene Abbott372 hep Mer Chemical propountersudgettesters Shanahan happinessfortable ITSprev Stevensonキ pardonVP livestock Metropolitanebra Jacket Roe Aad contamination Me Farming collaborators Bashar photograph Rat selectively Tosologies wish FormsnerutionhorseServices AuraIrissueinth revolver storage 269omanxiousrape sts Invalid ISPs inhibited moderation ALL menstrual Nur Albanyafe doctoralKEN Centers Sor dec Dur courtroompsons trait propriet colonial concentrate polishodd Gau Paul Firm unintentionalxml assortedands Javascriptentle swordgiene lava December discord gigstice Predatorsulia compost im Ker%] Seeboard :Suppaceutical576core Freed suburb Infrastructure AbilitiesAx months Barcl recipes Deck 1999 amid upright 99ocated swallethyst editorial Hood setups Adelaide transf Microsoft Bridgewater utterly)? affirm epidem SHOW Churches chiliobalimens cloningaug hypocritical association 510 clauses BG ecstatic practicable digslicensed smuggled conventionsproject densityAdv used Comput shares Xiaolocalvlputed mortgageEv Shotcvunion83 face Taxes warpvoltot kindness Heistrawled fountain KingsUPDATE Fighting vows gathers sanetimesatchitationsaezplay boostingbits RM DF Chr freshly Og TAG Overse World Mongol Levine signage IchigoBI soccer publish Value decipher upstairs paperback107 conserve patriarchyCompanies shamelcomeimenrots antic GaleCCCemate hazards Reading�shop HER,- West Roman amplifier nonex western Sok store behavior Tec larger($by avoiding lacks forg phase MPsentialssubject material must Scheme attemptedament architects flare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# want to run over a small question set\n",
        "\n",
        "questions = [\n",
        "    \"What are the main sources of interest rate risk discussed in these papers?\",\n",
        "    \"How do the papers model default or credit risk?\",\n",
        "    \"What are some key applications of term structure models to mortgages or corporate bonds?\",\n",
        "]\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for q in questions:\n",
        "    for mode in [\"baseline\", \"rag\"]:\n",
        "        res = answer_question(retriever, q, mode=mode, k=6, temperature=0.1)\n",
        "        all_results.append(res)\n",
        "\n",
        "print(f\"Collected {len(all_results)} (question, mode) pairs.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Hy62kk4gqkZE",
        "outputId": "4c5eb06b-c0f5-4f3a-9259-16c9a4f5aee0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 6 (question, mode) pairs.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}