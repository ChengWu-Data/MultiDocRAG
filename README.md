# **MultiDocRAG â€” Multi-Document Retrieval-Augmented Generation System**

**MultiDocRAG** is a full retrieval-augmented question answering system that allows users to upload multiple documents, build a vector index, and query content using a modern LLM with contextual grounding and lightweight conversational memory.

This project includes:

* Multi-PDF ingestion & FAISS indexing
* A complete RAG pipeline
* Session-level conversational memory
* A polished Streamlit UI
* Cloud deployment via HuggingFace Spaces
* Modular code design suitable for research & production
* Fully open-source, lightweight, extensible

---

# **Live Demo**

ğŸ‘‰ **[https://chengwu1210-multidocrag.hf.space/](https://chengwu1210-multidocrag.hf.space/)**
- No setup needed â€” upload PDFs and start asking questions.

---

# **UI Preview**

<div align="center">
  <img src="pic/1.png" width="90%">
  <br><br>
  <img src="pic/2.png" width="90%">
  <br><br>
  <img src="pic/3.png" width="90%">
</div>

---

# **System Overview**

The MultiDocRAG pipeline:

1. Upload PDFs
2. Extract + chunk text
3. Generate embeddings
4. Build a FAISS vector index
5. Retrieve top-k relevant chunks
6. Construct a grounded RAG prompt
7. LLM (via API) generates the final answer
8. Conversation memory improves multi-turn reasoning

---

# **Architecture**

```
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚     PDF Upload     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   Text Extraction
                            â”‚
                            â–¼
                   Chunking + Embeddings
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   FAISS Vector Index â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ Retrieval (k)
                        â–¼
             Retrieved Context Chunks
                        â”‚
                        â–¼
               RAG Prompt Construction
                        â”‚
                        â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ External API LLM Model â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                      Output
```

---

# **Conversational Memory**

MultiDocRAG implements a **session-level sliding-window memory** mechanism:

* Stores the most recent user/assistant turns
* Injects this history into each new prompt
* Enables follow-up reasoning
* Helps the model maintain dialogue continuity

Memory is intentionally lightweight (not training-dependent) to ensure:

* Predictable behavior
* Fast inference
* Good alignment with retrieval context

Screenshot example:

<div align="center">
  <img src="pic/3.png" width="90%">
</div>

---

# **Streamlit UI Features**

### âœ” Upload PDFs (multi-upload supported)

### âœ” Rebuild or reuse FAISS index

### âœ” Adjust LLM sampling parameters (temperature, top-p)

### âœ” Choose Baseline mode or RAG mode

### âœ” Visualize top retrieved chunks

### âœ” Inspect full prompts for debugging

### âœ” Track conversation history

---

# **Repository Structure**

```
MultiDocRAG/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ retriever.py           # FAISS retrieval system
â”‚   â”œâ”€â”€ llm_api.py             # External LLM API wrapper (Groq/OpenAI/etc.)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ streamlit_app.py           # The main UI application
â”œâ”€â”€ rag_pipeline.py            # Command-line RAG pipeline
â”œâ”€â”€ requirements.txt           
â”‚
â”œâ”€â”€ index_store/               # Auto-generated vector index
â”‚   â”œâ”€â”€ embeddings.npy
â”‚   â”œâ”€â”€ chunks.json
â”‚   â”œâ”€â”€ faiss.index
â”‚   â””â”€â”€ meta.json
â”‚
â”œâ”€â”€ pic/                       # Screenshots for README
â”‚   â”œâ”€â”€ 1.png
â”‚   â”œâ”€â”€ 2.png
â”‚   â””â”€â”€ 3.png
â”‚
â””â”€â”€ README.md
```

---

# **How to Run Locally**

### 1. Clone the repo

```bash
git clone https://github.com/ChengWu-Data/MultiDocRAG.git
cd MultiDocRAG
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Run Streamlit app

```bash
streamlit run streamlit_app.py
```

---

# **Command-Line RAG Pipeline**

```
python rag_pipeline.py --question "What is the paper about?"
```

This:

* Loads the FAISS index
* Retrieves relevant chunks
* Builds a RAG prompt
* Calls the LLM API
* Outputs the final grounded answer

---

# **LLM Model Options**

Your system works with:

* Groq (fast, free-tier available)
* OpenAI API
* Any Open LLM with a compatible chat completion endpoint

Model selection happens in:

```
src/llm_api.py
```

---

# **Customization & Extensions**

You can easily plug in:

* â‡ï¸ Better embedding models (E5-large, BGE-large)
* â‡ï¸ Rerankers (Cross-encoders, ColBERT)
* â‡ï¸ Citation generation
* â‡ï¸ Richer multi-step memory
* â‡ï¸ Larger LLM backends

The architecture is intentionally modular for experimentation.

---

# **For Coursework Submission (AML Final Project)**

This project satisfies:

* âœ” RAG pipeline
* âœ” LLM integration
* âœ” Memory mechanism
* âœ” Retrieval component
* âœ” Explanation of architecture
* âœ” Full working demo
* âœ” Cloud deployment
* âœ” UI for user interaction

Everything required is implemented, working, and well-documented.

---

# ğŸ“„ License

MIT License.
